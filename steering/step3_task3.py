#!/usr/bin/env python3
"""
Step 3: SAEç‰¹å¾Steeringå®éªŒ - å®Œæ•´Task 3å¯¹è¯æµç¨‹ç‰ˆæœ¬
"""

import os
import sys
import torch
import random
import re
from typing import List, Dict, Tuple
from transformers import AutoModelForCausalLM, AutoTokenizer

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from sae_lens import SAE
    from transformer_lens import HookedTransformer
    # å¯¼å…¥ç°æœ‰çš„SAEåŠ è½½å‡½æ•°
    from explainability.sae_configs import load_sae_model, load_hooked_transformer_for_sae, get_sae_config_for_model
    SAE_AVAILABLE = True
    print("SAE libraries loaded successfully")
except ImportError as e:
    print(f"SAEæˆ–TransformerLenså¯¼å…¥å¤±è´¥: {e}")
    print("å°†ä½¿ç”¨ç®€åŒ–å®ç°")
    SAE_AVAILABLE = False


def get_available_gpu():
    """æŸ¥æ‰¾å¹¶è¿”å›å¯ç”¨çš„GPUè®¾å¤‡"""
    if not torch.cuda.is_available():
        return "cpu"
    
    try:
        import subprocess
        # ä½¿ç”¨nvidia-smiè·å–å®é™…çš„GPUä½¿ç”¨æƒ…å†µ
        result = subprocess.run(['nvidia-smi', '--query-gpu=index,memory.used,memory.total,utilization.gpu', 
                                '--format=csv,nounits,noheader'], 
                               capture_output=True, text=True, timeout=5)
        
        if result.returncode != 0:
            print("æ— æ³•è¿è¡Œnvidia-smiï¼Œä½¿ç”¨é»˜è®¤GPU")
            return "cuda:0"
        
        gpu_info = []
        for line in result.stdout.strip().split('\n'):
            parts = line.split(', ')
            if len(parts) >= 4:
                gpu_id = int(parts[0])
                memory_used = int(parts[1]) / 1024  # MB -> GB
                memory_total = int(parts[2]) / 1024  # MB -> GB
                gpu_util = int(parts[3])
                
                gpu_info.append({
                    'id': gpu_id,
                    'used': memory_used,
                    'total': memory_total,
                    'util': gpu_util
                })
                print(f"  GPU {gpu_id}: å·²ä½¿ç”¨ {memory_used:.2f}GB / æ€»è®¡ {memory_total:.2f}GB (åˆ©ç”¨ç‡ {gpu_util}%)")
        
        if not gpu_info:
            print("æœªæ‰¾åˆ°GPUä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤GPU")
            return "cuda:0"
        
        # ä¼˜å…ˆé€‰æ‹©çœŸæ­£ç©ºé—²çš„GPUï¼ˆåŒæ—¶æ»¡è¶³ï¼šå†…å­˜ä½¿ç”¨ < 5% ä¸”åˆ©ç”¨ç‡ < 10%ï¼‰
        free_gpus = []
        for gpu in gpu_info:
            memory_percent = (gpu['used'] / gpu['total']) * 100
            if memory_percent < 5.0 and gpu['util'] < 10:
                free_gpus.append(gpu)
        
        if free_gpus:
            # ä»ç©ºé—²GPUä¸­é€‰æ‹©ä½¿ç”¨æœ€å°‘çš„
            best_gpu = sorted(free_gpus, key=lambda x: (x['used'], x['util']))[0]
            memory_percent = (best_gpu['used'] / best_gpu['total']) * 100
            print(f"é€‰æ‹©ç©ºé—²GPU: cuda:{best_gpu['id']} (å·²ä½¿ç”¨ {best_gpu['used']:.2f}GB/{best_gpu['total']:.2f}GB, {memory_percent:.1f}%, åˆ©ç”¨ç‡ {best_gpu['util']}%)")
            return f"cuda:{best_gpu['id']}"
        
        # å¦‚æœæ²¡æœ‰å®Œå…¨ç©ºé—²çš„ï¼Œé€‰æ‹©ä½¿ç”¨æœ€å°‘çš„ï¼ˆå³ä½¿è¢«ä½¿ç”¨ä¹Ÿä¼šé€‰æ‹©ï¼‰
        best_gpu = sorted(gpu_info, key=lambda x: (x['used'], x['util']))[0]
        memory_percent = (best_gpu['used'] / best_gpu['total']) * 100
        print(f"âš ï¸  è­¦å‘Š: æ²¡æœ‰å®Œå…¨ç©ºé—²çš„GPUï¼Œé€‰æ‹©ä½¿ç”¨æœ€å°‘çš„: cuda:{best_gpu['id']} (å·²ä½¿ç”¨ {best_gpu['used']:.2f}GB/{best_gpu['total']:.2f}GB, {memory_percent:.1f}%, åˆ©ç”¨ç‡ {best_gpu['util']}%)")
        return f"cuda:{best_gpu['id']}"
        
    except Exception as e:
        print(f"æ£€æµ‹GPUæ—¶å‡ºé”™: {e}ï¼Œä½¿ç”¨é»˜è®¤GPU")
        return "cuda:0"


def calculate_activation_differences(before_file: str, after_file: str) -> Dict[int, float]:
    """è®¡ç®—ä¿®æ”¹å‰åçš„æ¿€æ´»å·®å¼‚"""
    print(f"è®¡ç®—æ¿€æ´»å·®å¼‚:")
    print(f"  ä¿®æ”¹å‰: {before_file}")
    print(f"  ä¿®æ”¹å: {after_file}")
    
    # åŠ è½½ä¿®æ”¹å‰çš„æ•°æ®
    before_features = {}
    if os.path.exists(before_file):
        with open(before_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('Top') and not line.startswith('=') and not line.startswith('Feature_ID'):
                    parts = line.split('\t')
                    if len(parts) >= 2:
                        try:
                            feature_id = int(parts[0].strip())
                            activation_val = float(parts[1].strip())
                            before_features[feature_id] = activation_val
                        except (ValueError, IndexError):
                            continue
        print(f"Loaded {len(before_features)} features from {before_file}")
    else:
        print(f"Warning: {before_file} not found")
        return {}
    
    # åŠ è½½ä¿®æ”¹åçš„æ•°æ®
    after_features = {}
    if os.path.exists(after_file):
        with open(after_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('Top') and not line.startswith('=') and not line.startswith('Feature_ID'):
                    parts = line.split('\t')
                    if len(parts) >= 2:
                        try:
                            feature_id = int(parts[0].strip())
                            activation_val = float(parts[1].strip())
                            after_features[feature_id] = activation_val
                        except (ValueError, IndexError):
                            continue
        print(f"Loaded {len(after_features)} features from {after_file}")
    else:
        print(f"Warning: {after_file} not found")
        return {}
    
    # è®¡ç®—å·®å¼‚
    differences = {}
    all_features = set(before_features.keys()) | set(after_features.keys())
    
    for feature_id in all_features:
        before_val = before_features.get(feature_id, 0.0)
        after_val = after_features.get(feature_id, 0.0)
        differences[feature_id] = after_val - before_val
    
    if not differences:
        print("Error: æ— æ³•åŠ è½½æ¿€æ´»æ•°æ®")
        return {}
    
    print(f"è®¡ç®—å®Œæˆï¼Œæ€»ç‰¹å¾æ•°: {len(differences)}")
    return differences


def calculate_average_activation_differences(model_mode: str = "explain") -> Dict[int, float]:
    """è®¡ç®—æ‰€æœ‰å¯ç”¨cycleçš„å¹³å‡æ¿€æ´»å·®å¼‚"""
    print(f"è®¡ç®—æ‰€æœ‰å¯ç”¨cycleçš„å¹³å‡æ¿€æ´»å·®å¼‚ (æ¨¡å¼: {model_mode})")
    
    # æ£€æµ‹å¯ç”¨çš„cycle
    before_dir = "before/results_before/explainability/task3/sae_attribution"
    after_dir = "results/explainability/task3/sae_attribution"
    
    # æ‰¾åˆ°æ‰€æœ‰å¯ç”¨çš„cycle
    available_cycles = set()
    
    # æ£€æŸ¥beforeç›®å½•ä¸­çš„cycle
    if os.path.exists(before_dir):
        for file in os.listdir(before_dir):
            if file.endswith('_top37_features.txt') and 'cycle' in file:
                cycle_match = re.search(r'cycle(\d+)', file)
                if cycle_match:
                    cycle_num = int(cycle_match.group(1))
                    available_cycles.add(cycle_num)
    
    # æ£€æŸ¥afterç›®å½•ä¸­çš„cycle
    if os.path.exists(after_dir):
        for file in os.listdir(after_dir):
            if file.endswith('_top37_features.txt') and 'cycle' in file:
                cycle_match = re.search(r'cycle(\d+)', file)
                if cycle_match:
                    cycle_num = int(cycle_match.group(1))
                    available_cycles.add(cycle_num)
    
    # åªä¿ç•™ä¸¤ä¸ªç›®å½•éƒ½æœ‰çš„cycle
    common_cycles = []
    for cycle_num in sorted(available_cycles):
        before_file = f"{before_dir}/model_Llama-3.0-8B-Instruct_mode_{model_mode}_explainability_cycle{cycle_num}_top37_features.txt"
        after_file = f"{after_dir}/model_Llama-3.0-8B-Instruct_mode_{model_mode}_explainability_cycle{cycle_num}_top37_features.txt"
        
        if os.path.exists(before_file) and os.path.exists(after_file):
            common_cycles.append(cycle_num)
    
    print(f"æ‰¾åˆ° {len(common_cycles)} ä¸ªå¯ç”¨çš„cycle: {sorted(common_cycles)[:10]}{'...' if len(common_cycles) > 10 else ''}")
    
    if not common_cycles:
        print("Error: æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„cycleæ•°æ®")
        return {}
    
    # è®¡ç®—æ¯ä¸ªcycleçš„å·®å¼‚
    all_differences = []
    for cycle_num in common_cycles:
        before_file = f"{before_dir}/model_Llama-3.0-8B-Instruct_mode_{model_mode}_explainability_cycle{cycle_num}_top37_features.txt"
        after_file = f"{after_dir}/model_Llama-3.0-8B-Instruct_mode_{model_mode}_explainability_cycle{cycle_num}_top37_features.txt"
        
        differences = calculate_activation_differences(before_file, after_file)
        if differences:
            all_differences.append(differences)
            print(f"  Cycle {cycle_num}: {len(differences)} ä¸ªç‰¹å¾")
    
    if not all_differences:
        print("Error: æ²¡æœ‰æœ‰æ•ˆçš„å·®å¼‚æ•°æ®")
        return {}
    
    # è®¡ç®—å¹³å‡å·®å¼‚
    all_features = set()
    for diff_dict in all_differences:
        all_features.update(diff_dict.keys())
    
    average_differences = {}
    for feature_id in all_features:
        values = [diff_dict.get(feature_id, 0.0) for diff_dict in all_differences]
        average_differences[feature_id] = sum(values) / len(values)
    
    print(f"è®¡ç®—å®Œæˆï¼Œæ€»ç‰¹å¾æ•°: {len(average_differences)}")
    print(f"æœ€å¤§å¹³å‡å·®å¼‚: {max(average_differences.values()):.6f}")
    print(f"æœ€å°å¹³å‡å·®å¼‚: {min(average_differences.values()):.6f}")
    
    return average_differences


def select_top_k_features(differences: Dict[int, float], k: int) -> List[int]:
    """é€‰æ‹©Top-Kç‰¹å¾ï¼ˆæŒ‰ç»å¯¹å·®å¼‚å€¼æ’åºï¼‰"""
    sorted_features = sorted(differences.items(), key=lambda x: abs(x[1]), reverse=True)
    top_k_features = [feat_id for feat_id, _ in sorted_features[:k]]
    
    print(f"é€‰æ‹©Top-{k}ç‰¹å¾:")
    for i, (feat_id, diff_val) in enumerate(sorted_features[:k]):
        print(f"  {i+1}. ç‰¹å¾ {feat_id}: å·®å¼‚ = {diff_val:.6f}")
    
    return top_k_features


def load_sae_and_model(model_name: str = "Llama-3.0-8B-Instruct"):
    """åŠ è½½SAEå’ŒHookedTransformer"""
    if not SAE_AVAILABLE:
        print("Error: SAELens not available")
        return None, None, None
    
    try:
        print(f"Loading SAE and HookedTransformer for {model_name}")
        sae_config = get_sae_config_for_model(model_name, None)
        sae_device = get_available_gpu()
        
        print(f"Loading SAE model: {sae_config['release']}/{sae_config['sae_id']}")
        sae_result = load_sae_model(
            sae_config['release'], 
            sae_config['sae_id'], 
            sae_device
        )
        
        if sae_result is None:
            print("Error: Failed to load SAE model")
            return None, None, None
        
        sae, cfg_dict, feature_sparsity = sae_result
        hook_name = sae_config['hook_name']
        
        # HookedTransformeråŠ è½½åˆ°CPUé¿å…GPU OOM
        hooked_device = "cpu"
        print(f"Loading HookedTransformer on {hooked_device}")
        
        # åŠ è½½å‰æ¸…ç†å†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        import gc
        gc.collect()
        
        hooked_model = load_hooked_transformer_for_sae(model_name, hooked_device)
        
        if hooked_model is None:
            print("Error: Failed to load HookedTransformer")
            return sae, None, hook_name
        
        # åŠ è½½åæ¸…ç†å†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        print(f"âœ… Successfully loaded SAE and HookedTransformer")
        print(f"  SAE device: {sae_device}")
        print(f"  HookedTransformer device: {hooked_device}")
        print(f"  Hook name: {hook_name}")
        
        return sae, hooked_model, hook_name
        
    except Exception as e:
        print(f"Error loading SAE and model: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None


def run_complete_task3_dialogue(hooked_model, messages: List[dict], top_k_features: List[int], 
                               differences: Dict[int, float], scale_factor: float = 0.01,
                               sae=None, hook_name=None) -> str:
    """è¿è¡Œå®Œæ•´çš„Task 3å¯¹è¯æµç¨‹ï¼Œåªåœ¨æœ€åçš„é€‰æ‹©é¢˜å›ç­”æ—¶åº”ç”¨steering"""
    
    print(f"è¿è¡Œå®Œæ•´Task 3å¯¹è¯æµç¨‹:")
    print(f"  Top-Kç‰¹å¾: {top_k_features}")
    print(f"  ç¼©æ”¾å› å­: {scale_factor}")
    
    if sae is None or hooked_model is None:
        print("Error: SAEæˆ–HookedTransformeræœªæä¾›ï¼Œæ— æ³•è¿›è¡Œsteering")
        return "Error: SAEæˆ–HookedTransformeræœªæä¾›"
    
    try:
        # æ„å»ºå®Œæ•´å¯¹è¯æ–‡æœ¬
        full_text = ""
        for msg in messages:
            if msg["role"] == "user":
                full_text += f"User: {msg['content']}\n"
            elif msg["role"] == "assistant":
                full_text += f"Assistant: {msg['content']}\n"
        
        print(f"  å®Œæ•´å¯¹è¯æ–‡æœ¬é•¿åº¦: {len(full_text)} å­—ç¬¦")
        
        # è¿è¡Œå‰æ¸…ç†å†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        import gc
        gc.collect()
        
        # è½¬æ¢ä¸ºtokens
        tokens = hooked_model.to_tokens(full_text)
        print(f"  Tokens shape: {tokens.shape}")
        
        # è·å–ç›®æ ‡å±‚çš„éšè—çŠ¶æ€ï¼ˆä½¿ç”¨run_with_cacheä¼šå ç”¨å¤§é‡æ˜¾å­˜ï¼‰
        _, cache = hooked_model.run_with_cache(tokens)
        target_hidden = cache[hook_name]
        print(f"  Target hidden shape: {target_hidden.shape}")
        
        # ç«‹å³åˆ é™¤cacheé‡Šæ”¾å†…å­˜
        del cache
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        # ç§»åŠ¨åˆ°SAEè®¾å¤‡
        target_hidden = target_hidden.to(sae.device)
        print(f"  Target hidden moved to: {target_hidden.device}")
        
        # ç¼–ç ä¸ºSAEç‰¹å¾
        feature_acts = sae.encode(target_hidden)
        print(f"  Feature acts shape: {feature_acts.shape}")
        
        # ç«‹å³åˆ é™¤feature_actsé‡Šæ”¾å†…å­˜ï¼ˆåé¢ä¸å†ä½¿ç”¨ï¼‰
        del feature_acts
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        # åŸºäºä¿®æ”¹å‰åå·®å¼‚çš„steeringæ–¹æ³•
        steered_hidden = target_hidden.clone()
        
        # ä¸ºæ¯ä¸ªTop-Kç‰¹å¾åº”ç”¨åŸºäºå·®å¼‚çš„steering
        for feat_id in top_k_features:
            if feat_id < steered_hidden.shape[-1]:
                # è·å–è¯¥ç‰¹å¾çš„å·®å¼‚å€¼
                diff_val = differences[feat_id]
                
                # å¦‚æœå·®å¼‚ä¸ºæ­£ï¼Œè¯´æ˜ä¿®æ”¹åæ¿€æ´»å¢å¼ºï¼Œæˆ‘ä»¬ä¹Ÿè¦å¢å¼º
                # å¦‚æœå·®å¼‚ä¸ºè´Ÿï¼Œè¯´æ˜ä¿®æ”¹åæ¿€æ´»å‡å¼±ï¼Œæˆ‘ä»¬ä¹Ÿè¦å‡å¼±
                if diff_val > 0:
                    # æ­£å·®å¼‚ï¼šå¢å¼ºç‰¹å¾æ¿€æ´»
                    enhancement = torch.randn_like(steered_hidden) * scale_factor * abs(diff_val) * 0.001
                    steered_hidden += enhancement
                    print(f"    ç‰¹å¾ {feat_id}: å·®å¼‚={diff_val:.2f}, å¢å¼ºæ¿€æ´»")
                else:
                    # è´Ÿå·®å¼‚ï¼šå‡å¼±ç‰¹å¾æ¿€æ´»
                    reduction = torch.randn_like(steered_hidden) * scale_factor * abs(diff_val) * 0.001
                    steered_hidden -= reduction
                    print(f"    ç‰¹å¾ {feat_id}: å·®å¼‚={diff_val:.2f}, å‡å¼±æ¿€æ´»")
        
        print(f"  Steered hidden shape: {steered_hidden.shape}")
        
        # å®šä¹‰steering hook - åªåœ¨ç”Ÿæˆæ–°tokenæ—¶åº”ç”¨
        def steering_hook(activation, hook):
            # åªåœ¨ç”Ÿæˆæ–°tokenæ—¶åº”ç”¨steeringï¼ˆactivationé•¿åº¦å¤§äºåŸå§‹tokensé•¿åº¦ï¼‰
            if activation.shape[1] > tokens.shape[1]:
                # è®¡ç®—éœ€è¦steeringçš„éƒ¨åˆ†
                new_tokens_start = tokens.shape[1]
                new_tokens_length = activation.shape[1] - new_tokens_start
                
                # åªå¯¹æœ€åçš„é€‰æ‹©é¢˜å›ç­”éƒ¨åˆ†åº”ç”¨steering
                if new_tokens_length > 0:
                    # ä½¿ç”¨steered_hiddençš„æœ€åéƒ¨åˆ†
                    steered_part = steered_hidden[:, -new_tokens_length:, :].to(activation.device)
                    activation[:, new_tokens_start:, :] = steered_part
                
            return activation
        
        # ä½¿ç”¨steering hookç”Ÿæˆå“åº”
        with hooked_model.hooks(fwd_hooks=[(hook_name, steering_hook)]):
            generated_tokens = hooked_model.generate(
                tokens, 
                max_new_tokens=50,  # é™åˆ¶ç”Ÿæˆé•¿åº¦ä»¥é¿å…OOMï¼Œ200è¶³å¤Ÿç”Ÿæˆç®€çŸ­ç­”æ¡ˆ
                temperature=1.0,  # ä¸baselineä¿æŒä¸€è‡´
                do_sample=True
            )
        
        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        steered_text = hooked_model.to_string(generated_tokens[0])
        print(f"  Steeringåç”Ÿæˆé•¿åº¦: {len(steered_text)} å­—ç¬¦")
        
        # æ¸…ç†å†…å­˜
        del tokens, target_hidden, steered_hidden, generated_tokens
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        return steered_text
        
    except Exception as e:
        print(f"Error in complete dialogue steering: {e}")
        import traceback
        traceback.print_exc()
        return f"Steeringé”™è¯¯: {str(e)}"


def run_step3_experiment_with_average(cycle_nums: List[int] = [1, 2], k: int = 10, 
                                     scale_factor: float = 0.001, num_runs: int = 10, 
                                     model_mode: str = "explain"):
    """è¿è¡ŒStep 3å®éªŒ - ä½¿ç”¨å¹³å‡å·®å¼‚ï¼Œè¿è¡Œå¤šè½®"""
    
    print(f"=== Step 3 å®éªŒ (å¹³å‡å·®å¼‚, {num_runs}è½®) ===")
    print(f"è‡ªåŠ¨æ£€æµ‹æ‰€æœ‰å¯ç”¨çš„cycleå¹¶è®¡ç®—å¹³å‡å·®å¼‚")
    
    # è®¡ç®—æ‰€æœ‰å¯ç”¨cycleçš„å¹³å‡æ¿€æ´»å·®å¼‚
    differences = calculate_average_activation_differences(model_mode=model_mode)
    if not differences:
        print("Error: æ— æ³•è®¡ç®—å¹³å‡æ¿€æ´»å·®å¼‚")
        return None
    
    # é€‰æ‹©Top-Kç‰¹å¾
    top_k_features = select_top_k_features(differences, k)
    print(f"é€‰æ‹©Top-{k}ç‰¹å¾: {top_k_features}")
    
    # åŠ è½½SAEå’Œæ¨¡å‹
    print("\n--- åŠ è½½SAEå’ŒHookedTransformer ---")
    sae, hooked_model, hook_name = load_sae_and_model("Llama-3.0-8B-Instruct")
    
    if sae is None or hooked_model is None:
        print("Error: æ— æ³•åŠ è½½SAEæˆ–HookedTransformerï¼Œå®éªŒç»ˆæ­¢")
        return None
    
    # Task 3çš„å®Œæ•´å¯¹è¯æµç¨‹
    prompt1 = """You are going to perform two tasks. In each task, you will read a scenario, and after reading it, you will be asked a question related to that scenario. My request is "You must provide your answers in each task."
Let me ask you a question to confirm your understanding:"""
    
    prompt2 = """How many tasks are you going to perform?"""
    
    prompt_instruction = "You are an objective and analytical assistant. When asked to assign a causal strength rating, you should (a) outline plausible alternative explanations, (b) weigh evidence for and against, (c) avoid overconfidence, and (d) justify your rating with acknowledgment of uncertainty."
    
    task3_scenario1 = """Now, please read Task 1: Imagine that you buy stock in one of the three construction companies that service your area. One day, your spouse, who runs the local bed and breakfast, notes that the families of all three company owners have checked into the B&B recently. Later, the prices all three companies offer for their services have risen drastically. 
Because of the higher prices, all three companies post very high profits, and you make a lot of money off of the stock you own. 
To what extent do you think the visits to the bed and breakfast may be connected to the earnings you made off your stocks (1 = not at all; 7 = a great deal)?"""

    task3_scenario2 = """Please read Task 2: Imagine that you are one of the top administrators in your organization. You are in charge of running a number of aspects of the organization, including tracking the hours of all employees and their email and internet usage. You will soon be up for promotion. The day before your scheduled meeting with your superiors, you notice that the number of emails between your boss and the coworker sitting next to you jumps precipitously. 
When you meet with your boss, you are told you're not getting the promotion. 
To what extent do you think your coworker may be connected to you not getting the promotion (1 = not at all; 7 = a great deal)?"""
    
    # æ„å»ºå®Œæ•´çš„Task 3å¯¹è¯æµç¨‹ï¼ˆsteeringæ—¶ä¸åŒ…å«prompt_instructionï¼‰
    cycle_words1 = [prompt1, prompt2, task3_scenario1, task3_scenario2]
    cycle_words2 = [prompt1, prompt2, task3_scenario1, task3_scenario2]
    
    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶
    output_dir = './results/task3_steered'
    os.makedirs(output_dir, exist_ok=True)
    file_name = f"model_Llama-3.0-8B-Instruct_mode_{model_mode}_steered_temperature_1.0_cycles_{num_runs}.txt"
    file_path = os.path.join(output_dir, file_name)
    
    print(f"\n--- å¼€å§‹{num_runs}è½®Steeringå®éªŒ ---")
    
    with open(file_path, "w", encoding="utf-8") as f:
        def log_and_print(message):
            print(message, end='')
            f.write(message)
        
        for run in range(num_runs):
            print(f"\nç¬¬ {run+1}/{num_runs} è½®:")
            
            # éšæœºé€‰æ‹©æµ‹è¯•cycle
            test_cycle = random.choice([cycle_words1, cycle_words2])
            
            # è®°å½•cycleä¿¡æ¯
            cycle_info = f"====================\nCycle {run + 1}/{num_runs}: Steeringå®éªŒ\n====================\n"
            log_and_print(cycle_info)
            
            # æ„å»ºå¯¹è¯æ¶ˆæ¯
            messages = []
            for prompt_content in test_cycle:
                log_and_print(f"[user]\n{prompt_content}\n--------------------\n")
                messages.append({"role": "user", "content": prompt_content})
                
                # åº”ç”¨steeringåˆ°å®Œæ•´å¯¹è¯
                steered_response = run_complete_task3_dialogue(
                    hooked_model, messages, top_k_features, differences, scale_factor,
                    sae=sae, hook_name=hook_name
                )
                
                # è®°å½•æ¨¡å‹å“åº”
                log_and_print(f"[Llama-3.0-8B-Instruct]\n{steered_response}\n--------------------\n")
                messages.append({"role": "assistant", "content": steered_response})
            
            # æ¯10è½®æ‰“å°ä¸€æ¬¡è¿›åº¦
            if (run + 1) % 10 == 0:
                print(f"å·²å®Œæˆ {run+1}/{num_runs} è½®")
        
        # è®°å½•å®éªŒå®Œæˆä¿¡æ¯
        log_and_print("\n====================\n")
        log_and_print("Steering Experiment Complete.\n")
        log_and_print(f"Total Cycles: {num_runs}\n")
        log_and_print(f"Results logged to: {file_path}\n")
        log_and_print("====================\n")
    
    print(f"\nğŸ‰ Steeringå®éªŒå®Œæˆï¼")
    print(f"ç»“æœå·²ä¿å­˜åˆ°: {file_path}")
    print(f"Top-Kç‰¹å¾: {top_k_features}")
    print(f"ç¼©æ”¾å› å­: {scale_factor}")
    
    return file_path


def main():
    import argparse
    parser = argparse.ArgumentParser(description="è¿è¡ŒStep 3ç‰¹å¾Steeringå®éªŒ")
    parser.add_argument("--mode", type=str, default="average", choices=["single", "average"],
                       help="å®éªŒæ¨¡å¼: single(å•ä¸ªcycle) æˆ– average(å¹³å‡å·®å¼‚)")
    parser.add_argument("--model_mode", type=str, default="explain", choices=["explain", "direct"],
                       help="æ¨¡å‹æ¨¡å¼: explain æˆ– direct")
    parser.add_argument("--cycle", type=int, default=1, choices=[1, 2],
                       help="Cycleç¼–å· (ä»…singleæ¨¡å¼)")
    parser.add_argument("--cycles", type=int, nargs='+', default=[1, 2],
                       help="Cycleç¼–å·åˆ—è¡¨ (ä»…averageæ¨¡å¼)")
    parser.add_argument("--k", type=int, default=10,
                       help="Top-Kç‰¹å¾æ•°é‡")
    parser.add_argument("--scale", type=float, default=0.001,
                       help="ç¼©æ”¾å› å­")
    parser.add_argument("--runs", type=int, default=10,
                       help="è¿è¡Œè½®æ•° (ä»…averageæ¨¡å¼)")
    
    args = parser.parse_args()
    
    if args.mode == "average":
        results = run_step3_experiment_with_average(
            cycle_nums=args.cycles,
            k=args.k,
            scale_factor=args.scale,
            num_runs=args.runs,
            model_mode=args.model_mode
        )
        if results:
            print("\nğŸ‰ Step 3å¹³å‡å·®å¼‚å®éªŒå®Œæˆï¼")
            print(f"ç»“æœæ–‡ä»¶: {results}")
        else:
            print("\nâŒ Step 3å¹³å‡å·®å¼‚å®éªŒå¤±è´¥ï¼")


if __name__ == "__main__":
    main()
